{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Feature Points\n",
    "\n",
    "To draw the feature points, I obtain each individual feature point by selecting it using the *face* parameter that is passed into the **drawFeaturePoints** function.  I was able to analyze this object by setting a breakpoint within the function while running the application and inspecting the face parameter. Using the canvas context object I drew circles for each feature point using the *x* and *y* member variables of the featurepoint object.\n",
    "\n",
    "~~~~~\n",
    "function drawFeaturePoints(canvas, img, face) {\n",
    "  // Obtain a 2D context object to draw on the canvas\n",
    "  var ctx = canvas.getContext('2d');\n",
    "  \n",
    "  ctx.fillStyle = 'red';\n",
    "  ctx.strokeStyle = 'blue';\n",
    "  // Loop over each feature point in the face\n",
    "  for (var id in face.featurePoints) {\n",
    "    var featurePoint = face.featurePoints[id];\n",
    "    \n",
    "    ctx.beginPath();\n",
    "    ctx.arc(featurePoint.x,featurePoint.y,3,0,2*Math.PI);\n",
    "    ctx.stroke();\n",
    "  }\n",
    "}\n",
    "~~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dominant Emoji\n",
    "\n",
    "To display the dominant emoji, I obtained this from the face parameter passed into the function. I randomly chose the first feature point as the reference point for where I wanted to draw the dominant emoji. Once I noticed that the first feature point was located on the left side of the face displayed, I decided to move it to the left by subtracting 10 from the feature point's x coordinate.  This placed it at a comformatable location that is easily visible.\n",
    "\n",
    "~~~~\n",
    "function drawEmoji(canvas, img, face) {\n",
    "  // Obtain a 2D context object to draw on the canvas\n",
    "  var ctx = canvas.getContext('2d');\n",
    "  ctx.font = '30px serif';\n",
    "  ctx.fillText(face.emojis.dominantEmoji,face.featurePoints[0].x -10,face.featurePoints[0].y);\n",
    "}\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mimic Game\n",
    "\n",
    "In implementing the Mimic Game, my first objective was defining the **update** function.  My initial goal was to display the target emoji and have the score increment if the dominantEmoji matched the target emoji.  Once I had this mechanism working, I focused on getting the scoring system correct.  This took some experimentation as I had to get back accustomed to the fact that the update function was being constantly called when a face was detected (due to the event listener). \n",
    "\n",
    "~~~~\n",
    "function update(face,timestamp){\n",
    "  \n",
    "  var emoji_target = emojis[emoji_face % emojis.length];\n",
    "  var audio_file = correct_audio[correct_audio_index % correct_audio.length];\n",
    "  setTargetEmoji(emoji_target);\n",
    "  \n",
    "  if(toUnicode(face.emojis.dominantEmoji) == emoji_target){\n",
    "    var audio = new Audio(audio_file);\n",
    "    audio.play();\n",
    "    last_update_time = 0;\n",
    "    emoji_correct = true;\n",
    "    correct++;\n",
    "    correct_audio_index++;\n",
    "  }\n",
    "}\n",
    "~~~~\n",
    "\n",
    "This led me to the final task which was getting the timing and synchronization correct.  I chose to maintain a second timestamp variable and conditionally call the **update** function based on a time difference or whether the target emoji was mimic correctly.\n",
    " \n",
    " \n",
    "~~~~\n",
    "if(timestamp - last_update_time > 8 || emoji_correct){\n",
    "      emoji_face = Math.floor(Math.random() * 34);\n",
    "      var current_correct = correct;\n",
    "      last_update_time = timestamp;\n",
    "      total++; \n",
    "      if(!emoji_correct){\n",
    "        var audio_file = incorrect_audio[incorrect_audio_index % incorrect_audio.length];\n",
    "        var audio = new Audio(audio_file)\n",
    "        audio.play();\n",
    "        incorrect_audio_index++;\n",
    "      }\n",
    "      emoji_correct = false;\n",
    "    }\n",
    "\n",
    "    update(faces[0],timestamp);\n",
    "    setScore(correct,total);\n",
    "  }\n",
    "~~~~\n",
    "To put the final touches on the game, I decided to play some audio based on whether the emoji was mimic correctly or not.  I decided to use some soundbites from one of my favorite shows [Rick and Morty](http://www.adultswim.com/videos/rick-and-morty/).  I obtained the wav files from [here](https://peal.io/soundboards/rick-and-morty).  I inspected the speaker element objects from that website and copied the element that held the wav file source location.  Luckily, after I navigated to the source location, The wav file was downloadable.  So the game will play some audio based on whether the dominant emoji is correct or incorrect.\n",
    "\n",
    "\n",
    "One thing that I would like to note is that the Affectiva API doesn't work particularly well on dark faces such as mine. It would pick up the pink of my inner lip as my tongue being out and when I actually stuck my tongue out, it couldn't recogninze anything at all.  I also had a difficult time detecting my face with my glasses on. I eventually took my glasses off when testing to get better results. I spent most of the time trying to get the API to recognize my face more than anything."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:aind-dl]",
   "language": "python",
   "name": "conda-env-aind-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
